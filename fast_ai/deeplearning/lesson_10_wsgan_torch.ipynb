{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run env_setup.py\n",
    "%matplotlib notebook\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import FloatTensor, nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import yama\n",
    "import yama.vision as yamavision\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yama.vision.datasets import LocalStorage, PaperSpaceGradientStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = LocalStorage(os.path.abspath('../../_data'))\n",
    "#storage = PaperSpaceGradientStorage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, image_size, noise_size = 64, (64, 64), 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env http_proxy=http://127.0.0.1:1087\n",
    "%env https_proxy=http://127.0.0.1:1087"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.abspath('../../_data/cifar10')\n",
    "\n",
    "data = torchvision.datasets.CIFAR10(root=data_path, download=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(image_size),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = os.path.abspath('../../_data/lsun/')\n",
    "data_path = '../../_data/lsun'\n",
    "data = yama.vision.datasets.LSUN(storage, classes=['bedroom_train'],\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(image_size),\n",
    "        torchvision.transforms.CenterCrop(image_size),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(data, batch_size, shuffle=True, num_workers=os.cpu_count())\n",
    "n = len(dataloader); n\n",
    "sample_x, sample_y = data[0]\n",
    "input_size = tuple(sample_x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGan_D(nn.Module):\n",
    "    def __init__(self, input_size, feature_num, mid_layers=1):\n",
    "        super().__init__()\n",
    "        img_channels, img_size = input_size[0], input_size[1:]\n",
    "        assert img_size[0] == img_size[1]\n",
    "        img_size = img_size[0]\n",
    "    \n",
    "        main = nn.Sequential()\n",
    "\n",
    "        def conv_block(name, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "            main.add_module('{name}_conv_{in_channels}_{out_channels}_{kernel_size}'.format(**locals()),\n",
    "                           nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n",
    "            main.add_module('{name}_batchnorm'.format(**locals()),\n",
    "                           nn.BatchNorm2d(out_channels))\n",
    "            main.add_module('{name}_LeakyRelu'.format(**locals()),\n",
    "                           nn.LeakyReLU(0.2))\n",
    "            return out_channels\n",
    "        \n",
    "        last_channels = conv_block('b1', img_channels, feature_num,\n",
    "                                   kernel_size=4, stride=2, padding=1)\n",
    "        n_feature = feature_num // 2\n",
    "        for l in range(mid_layers):\n",
    "            last_channels = conv_block('mid-{}'.format(l), last_channels,\n",
    "                                      n_feature, kernel_size=3, padding=1)\n",
    "        feature_map_size = img_size // 2\n",
    "        while feature_map_size > 4:\n",
    "            last_channels = conv_block('pyramid-{}'.format(feature_map_size), last_channels,\n",
    "                                       last_channels*2, kernel_size=4, stride=2, padding=1)\n",
    "            feature_map_size //= 2\n",
    "        main.add_module('final-{}-conv'.format(last_channels),\n",
    "                        nn.Conv2d(last_channels, 1, feature_map_size, bias=False))\n",
    "        self.main = main\n",
    "    \n",
    "    def forward(self, images):\n",
    "        out = self.main(images).mean(0)\n",
    "        return out.view(1)\n",
    "        \n",
    "\n",
    "class DCGan_G(nn.Module):\n",
    "    def __init__(self, noise_len, out_size, feature_num, mid_layers=1):\n",
    "        super().__init__()\n",
    "        main = nn.Sequential()\n",
    "        def deconv_block(name, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "            main.add_module(\"{name}_conv_{in_channels}_{out_channels}_{kernel_size}\".format(**locals()),\n",
    "                            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding))\n",
    "            main.add_module(\"{}_batch_norm\".format(name), nn.BatchNorm2d(out_channels))\n",
    "            main.add_module(\"{}_relu\".format(name), nn.ReLU())\n",
    "            return out_channels\n",
    "        img_channels, img_size = out_size[0], out_size[1:]\n",
    "        assert img_size[0] == img_size[1]\n",
    "        img_size = img_size[0]\n",
    "        assert img_size % 16 == 0\n",
    "        \n",
    "        tmp_img_size, feature_num = 4, feature_num//2\n",
    "        while tmp_img_size != img_size:\n",
    "            feature_num *= 2\n",
    "            tmp_img_size *= 2\n",
    "        \n",
    "        last_channels = deconv_block('init', noise_len, feature_num, 4)\n",
    "        \n",
    "        feature_size = 4\n",
    "        while feature_size < img_size//2:\n",
    "            last_channels = deconv_block('pyramid_{}'.format(feature_size), last_channels,\n",
    "                                        last_channels // 2, 4, 2, 1)\n",
    "            feature_size *= 2\n",
    "        for l in range(mid_layers):\n",
    "            last_channels = deconv_block('mid_{}'.format(l), last_channels,\n",
    "                                        last_channels, 3, 1, 1)\n",
    "        main.add_module('final_convt',\n",
    "                        nn.ConvTranspose2d(last_channels, img_channels, 4, 2, 1))\n",
    "        main.add_module('final_tanh', nn.Tanh())\n",
    "        \n",
    "        self.main = main\n",
    "    \n",
    "    def forward(self, in_noise):\n",
    "        return self.main(noise)\n",
    "\n",
    "def make_noise(batch_size, noise_channels):\n",
    "    return Variable(torch.randn(batch_size, noise_channels, 1, 1))\n",
    "\n",
    "def make_trainable(m, b=True):\n",
    "    for v in m.parameters(): v.require_grad = b\n",
    "    \n",
    "def train(D, G, opt_D, opt_G, loader, epochs, batch_size, noise_channels, first=True, use_gpu=False):\n",
    "    \n",
    "    n = len(loader)\n",
    "    make_trainable(D)\n",
    "    for ep in range(epochs):\n",
    "        gen_iter = 0\n",
    "        d_iter = 0\n",
    "        d_iter_tgt = 0\n",
    "        bar = tqdm(loader, desc='{ep}/{epochs}'.format(**locals()))\n",
    "        for real in bar:\n",
    "            real_x, real_y = real\n",
    "            if use_gpu:\n",
    "                real_x, real_y = real_x.cuda(), real_y.cuda()\n",
    "            real_x = Variable(real_x)\n",
    "            if d_iter_tgt == 0 or d_iter >= d_iter_tgt:\n",
    "                is_warm_up = first and gen_iter < 25\n",
    "                if is_warm_up or gen_iter % 500 == 0:\n",
    "                    d_iter_tgt = 100\n",
    "                else:\n",
    "                    d_iter_tgt = 5\n",
    "                d_iter = 0\n",
    "            fake = G(make_noise(real_x.size()[0], noise_channels)).detach()\n",
    "            err = D(real_x) - D(fake)\n",
    "            err.backward()\n",
    "            opt_D.step()\n",
    "            bar.set_postfix(loss=float(err.data))\n",
    "            \n",
    "            print('iter', d_iter, d_iter_tgt)\n",
    "            d_iter += 1\n",
    "            if d_iter == d_iter_tgt:\n",
    "                make_trainable(D, False)\n",
    "                fake = G(make_noise(batch_size, noise_channels))\n",
    "                D(fake).backward()   # Generator want to cheat D -> 0 (means real)\n",
    "                opt_G.step()\n",
    "                gen_iter += 1\n",
    "                make_trainable(D, True)\n",
    "\n",
    "def weight_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        m.weight.data.normal_(0, 0.02)\n",
    "    elif isinstance(m, (nn.BatchNorm2d,)):\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=DCGan_D(input_size, 64, 1).cuda()(Variable(p[0].cuda())); x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = make_noise(batch_size, noise_size).cuda()\n",
    "DCGan_G(noise_size, input_size, feature_num=64, mid_layers=1).cuda()(Variable(p[0].cuda())).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(noise).data.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = DCGan_G(noise_size, input_size, feature_num=64, mid_layers=1).cuda()\n",
    "d = DCGan_D(input_size, feature_num=64, mid_layers=1).cuda()\n",
    "\n",
    "opt_d = torch.optim.RMSprop(d.parameters())\n",
    "opt_g = torch.optim.RMSprop(g.parameters())\n",
    "\n",
    "for m in [d, g]:\n",
    "    m.apply(weight_init)\n",
    "train(d, g, opt_d, opt_g, dataloader,\n",
    "      epochs=100, batch_size=32, noise_channels=noise_size, use_gpu=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
