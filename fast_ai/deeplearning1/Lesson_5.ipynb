{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run env_setup.py\n",
    "import keras\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "from lessdeep.datasets.stanford import imdb\n",
    "import lessdeep as ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'it', 'in', 'i']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(imdb)\n",
    "idx = imdb.get_word_index()\n",
    "idx_arr = sorted(idx, key=idx.get)\n",
    "idx_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {i: w for w, i in idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use pre-calculated vocabulary\n"
     ]
    }
   ],
   "source": [
    "(x_train, labels_train), (x_test, labels_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sentence(arr):\n",
    "    return ' '.join([idx2word[i] for i in arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell high 's satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i m here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it is n't\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_sentence(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "\n",
    "trn = [np.array([min(i, vocab_size - 1) for i in s]) for s in x_train]\n",
    "tst = [np.array([min(i, vocab_size - 1) for i in s]) for s in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 242.54411999999999, 2527]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_lens = np.array(list(map(len, trn)))\n",
    "[trn_lens.min(), trn_lens.mean(), trn_lens.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_max_len = 600\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "trn = pad_sequences(trn, maxlen=seq_max_len, value=pad_value)\n",
    "tst = pad_sequences(tst, maxlen=seq_max_len, value=pad_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 600)\n",
      "(25000, 600)\n"
     ]
    }
   ],
   "source": [
    "print(trn.shape)\n",
    "print(tst.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 600, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 19200)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               1920100   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 2,080,201\n",
      "Trainable params: 2,080,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_features = 32\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, output_dim=word_features, input_length=seq_max_len),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(100),\n",
    "    keras.layers.Dropout(0.7),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 3s 132us/step - loss: 0.4358 - acc: 0.7809 - val_loss: 0.2923 - val_acc: 0.8762\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 3s 126us/step - loss: 0.1671 - acc: 0.9387 - val_loss: 0.3414 - val_acc: 0.8632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a5f3dd2a58>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(tst, labels_test), epochs=2, batch_size=64, callbacks=[ld.utils.tf_board()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "Convolve neural network is good for sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 600, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 600, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 596, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 298, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 9536)              0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 9536)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               953700    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,118,953\n",
      "Trainable params: 1,118,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_features = 32\n",
    "cnn_model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, output_dim=word_features, input_length=seq_max_len),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Conv1D(filters=32, kernel_size=5, activation='relu'),\n",
    "    keras.layers.MaxPool1D(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.7),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "cnn_model.compile(keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/8\n",
      "25000/25000 [==============================] - 3s 101us/step - loss: 0.6929 - acc: 0.5157 - val_loss: 0.6829 - val_acc: 0.6673\n",
      "Epoch 2/8\n",
      "25000/25000 [==============================] - 2s 94us/step - loss: 0.4325 - acc: 0.7995 - val_loss: 0.2876 - val_acc: 0.8837\n",
      "Epoch 3/8\n",
      "25000/25000 [==============================] - 2s 94us/step - loss: 0.2741 - acc: 0.8954 - val_loss: 0.2626 - val_acc: 0.8928\n",
      "Epoch 4/8\n",
      "25000/25000 [==============================] - 2s 94us/step - loss: 0.2315 - acc: 0.9115 - val_loss: 0.2683 - val_acc: 0.8900\n",
      "Epoch 5/8\n",
      "25000/25000 [==============================] - 2s 95us/step - loss: 0.2114 - acc: 0.9193 - val_loss: 0.2830 - val_acc: 0.8849\n",
      "Epoch 6/8\n",
      "25000/25000 [==============================] - 2s 94us/step - loss: 0.1877 - acc: 0.9297 - val_loss: 0.2791 - val_acc: 0.8895\n",
      "Epoch 7/8\n",
      "25000/25000 [==============================] - 2s 95us/step - loss: 0.1836 - acc: 0.9313 - val_loss: 0.2973 - val_acc: 0.8857\n",
      "Epoch 8/8\n",
      "25000/25000 [==============================] - 2s 95us/step - loss: 0.1698 - acc: 0.9348 - val_loss: 0.2996 - val_acc: 0.8887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a62d3135c0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(trn, labels_train, validation_data=(tst, labels_test), shuffle=True, epochs=8, batch_size=128, callbacks=[ld.utils.tf_board('cnn')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function tf_board at 0x000002A5D61BB1E0>\n",
      "<function tf_board at 0x000002A5D61BB158>\n"
     ]
    }
   ],
   "source": [
    "print(ld.utils.tf_board)\n",
    "reload(ld)\n",
    "reload(ld.utils)\n",
    "print(ld.utils.tf_board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lessdeep.utils.word_vec import glove\n",
    "g_words, g_vecs = glove()\n",
    "g_word2idx = {w:i for i, w in enumerate(g_words)}\n",
    "g_features = len(g_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(vecs, word2idx, old_vocab):\n",
    "    feature_num = len(vecs[0])\n",
    "    res_emb = np.empty((len(old_vocab), feature_num), dtype=type(vecs[0][0]))\n",
    "    for i, word in enumerate(old_vocab):\n",
    "        if word in word2idx:\n",
    "            res_emb[i, :] = vecs[word2idx[word]]\n",
    "        else:\n",
    "            print('[{0}]word: '.format(i) + word + ' not inside new vocabulary')\n",
    "            res_emb[i, :] = np.random.normal(scale=0.6, size=(feature_num,))\n",
    "    # the padding value\n",
    "    res_emb[pad_value, :] = np.random.normal(scale=0.6, size=(feature_num,))\n",
    "\n",
    "    return res_emb\n",
    "\n",
    "g_emb = create_emb(g_vecs, g_word2idx, idx_arr[:vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 600, 50)           250000    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 600, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 600, 32)           8032      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 300, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 9600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 9600)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               960100    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,218,233\n",
      "Trainable params: 968,233\n",
      "Non-trainable params: 250,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "glove_cnn_model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, output_dim=g_features, input_length=seq_max_len,\n",
    "                           weights=[g_emb/3], trainable=False),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Conv1D(filters=32, kernel_size=5, activation='relu', padding='same'),\n",
    "    keras.layers.MaxPool1D(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dropout(0.7),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "glove_cnn_model.compile(keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "glove_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/8\n",
      "25000/25000 [==============================] - 2s 89us/step - loss: 0.3254 - acc: 0.8588 - val_loss: 0.3748 - val_acc: 0.8379\n",
      "Epoch 2/8\n",
      "25000/25000 [==============================] - 2s 88us/step - loss: 0.3274 - acc: 0.8584 - val_loss: 0.3683 - val_acc: 0.8389\n",
      "Epoch 3/8\n",
      "25000/25000 [==============================] - 2s 89us/step - loss: 0.3123 - acc: 0.8630 - val_loss: 0.3768 - val_acc: 0.8367\n",
      "Epoch 4/8\n",
      "25000/25000 [==============================] - 2s 89us/step - loss: 0.3129 - acc: 0.8631 - val_loss: 0.3706 - val_acc: 0.8391\n",
      "Epoch 5/8\n",
      "25000/25000 [==============================] - 2s 88us/step - loss: 0.3132 - acc: 0.8636 - val_loss: 0.3697 - val_acc: 0.8394\n",
      "Epoch 6/8\n",
      "25000/25000 [==============================] - 2s 88us/step - loss: 0.3093 - acc: 0.8667 - val_loss: 0.3686 - val_acc: 0.8416\n",
      "Epoch 7/8\n",
      "25000/25000 [==============================] - 2s 89us/step - loss: 0.3064 - acc: 0.8664 - val_loss: 0.3698 - val_acc: 0.8400\n",
      "Epoch 8/8\n",
      "25000/25000 [==============================] - 2s 87us/step - loss: 0.3068 - acc: 0.8663 - val_loss: 0.3702 - val_acc: 0.8408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25640a99cf8>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_cnn_model.fit(trn, labels_train, validation_data=(tst, labels_test), shuffle=True, epochs=8, batch_size=128, callbacks=[ld.utils.tf_board('glv_cnn')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Continue last epoch in both tensorboard and training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
